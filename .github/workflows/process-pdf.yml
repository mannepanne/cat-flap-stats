# ABOUT: GitHub Actions workflow for processing uploaded PDF files
# ABOUT: Triggered by CloudFlare webhook, processes PDF using Python extractor, updates dataset

name: Process Cat Flap PDF

on:
  repository_dispatch:
    types: [process-pdf]

jobs:
  process-pdf:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.13
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download PDF from CloudFlare KV
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        KV_NAMESPACE_ID: ${{ secrets.KV_NAMESPACE_ID }}
      run: |
        # Extract file ID from webhook payload
        FILE_ID="${{ github.event.client_payload.file_id }}"
        FILENAME="${{ github.event.client_payload.filename }}"
        
        echo "Processing file: $FILENAME (ID: $FILE_ID)"
        
        # Download PDF from CloudFlare KV using REST API
        curl -X GET "https://api.cloudflare.com/client/v4/accounts/$CLOUDFLARE_ACCOUNT_ID/storage/kv/namespaces/$KV_NAMESPACE_ID/values/upload:$FILE_ID" \
          -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
          -H "Content-Type: application/octet-stream" \
          --output "temp_upload.pdf"
        
        # Verify PDF was downloaded
        if [ ! -f "temp_upload.pdf" ]; then
          echo "Failed to download PDF file"
          exit 1
        fi
        
        echo "PDF downloaded successfully: $(wc -c < temp_upload.pdf) bytes"
        
    - name: Process PDF with extractor
      run: |
        echo "Processing PDF with cat_flap_extractor_v5.py"
        
        # Run the extractor on the downloaded PDF
        python3 cat_flap_extractor_v5.py temp_upload.pdf --format both --output processed_data
        
        # Check if processing succeeded
        if [ ! -f "processed_data.csv" ] || [ ! -f "processed_data.json" ]; then
          echo "PDF processing failed - output files not found"
          exit 1
        fi
        
        echo "PDF processing completed successfully"
        echo "CSV file size: $(wc -c < processed_data.csv) bytes"
        echo "JSON file size: $(wc -c < processed_data.json) bytes"
        
    - name: Backup existing dataset
      run: |
        # Create backup directory with timestamp
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        mkdir -p "dataset_backups/$TIMESTAMP"
        
        # Backup existing dataset files if they exist
        if [ -f "master_dataset.csv" ]; then
          cp master_dataset.csv "dataset_backups/$TIMESTAMP/"
          echo "Backed up existing CSV dataset"
        fi
        
        if [ -f "master_dataset.json" ]; then
          cp master_dataset.json "dataset_backups/$TIMESTAMP/"
          echo "Backed up existing JSON dataset"
        fi
        
    - name: Check for duplicates and merge with master dataset
      run: |
        echo "Checking for duplicates and merging with master dataset"
        
        # If master datasets don't exist, initialize them
        if [ ! -f "master_dataset.csv" ]; then
          echo "Initializing master CSV dataset"
          cp processed_data.csv master_dataset.csv
        else
          echo "Checking for duplicate data before merging"
          
          # Use Python to detect and handle duplicates intelligently
          python3 -c "
import pandas as pd
import json
from datetime import datetime

print('Loading datasets for duplicate detection...')

# Load existing master dataset
master_df = pd.read_csv('master_dataset.csv')
print(f'Master dataset has {len(master_df)} sessions')

# Load new data
new_df = pd.read_csv('processed_data.csv')
print(f'New data has {len(new_df)} sessions')

# Create a composite key for duplicate detection
# Using date + session_number + exit_time + entry_time + duration
def create_session_key(df):
    return df['date'].astype(str) + '_' + df['session_number'].astype(str) + '_' + df['exit_time'].astype(str) + '_' + df['entry_time'].astype(str)

if len(master_df) > 0:
    master_df['session_key'] = create_session_key(master_df)
    new_df['session_key'] = create_session_key(new_df)
    
    # Find duplicates
    duplicates = new_df[new_df['session_key'].isin(master_df['session_key'])]
    unique_new = new_df[~new_df['session_key'].isin(master_df['session_key'])]
    
    print(f'Found {len(duplicates)} duplicate sessions')
    print(f'Found {len(unique_new)} new unique sessions')
    
    if len(duplicates) > 0:
        print('Duplicate sessions found:')
        for _, dup in duplicates.iterrows():
            print(f'  - {dup[\"date\"]} session {dup[\"session_number\"]} ({dup[\"exit_time\"]} - {dup[\"entry_time\"]})')
    
    # Only append unique new sessions
    if len(unique_new) > 0:
        # Remove the session_key column before saving
        unique_new = unique_new.drop('session_key', axis=1)
        
        # Append unique sessions to master dataset
        combined_df = pd.concat([master_df.drop('session_key', axis=1), unique_new], ignore_index=True)
        combined_df.to_csv('master_dataset.csv', index=False)
        print(f'Updated master dataset now has {len(combined_df)} sessions')
    else:
        print('No new unique sessions to add - master dataset unchanged')
        # Remove session_key from master and resave to clean it up
        master_df.drop('session_key', axis=1).to_csv('master_dataset.csv', index=False)
else:
    # Master is empty, just copy new data
    new_df.to_csv('master_dataset.csv', index=False)
    print(f'Initialized master dataset with {len(new_df)} sessions')

# Create processing summary
with open('duplicate_report.txt', 'w') as f:
    f.write(f'Duplicate Detection Report\\n')
    f.write(f'========================\\n')
    f.write(f'New sessions processed: {len(new_df)}\\n')
    f.write(f'Duplicate sessions found: {len(duplicates) if len(master_df) > 0 else 0}\\n')
    f.write(f'Unique new sessions added: {len(unique_new) if len(master_df) > 0 else len(new_df)}\\n')
    f.write(f'Total sessions in dataset: {len(pd.read_csv(\"master_dataset.csv\"))}\\n')
"
        fi
        
        if [ ! -f "master_dataset.json" ]; then
          echo "Initializing master JSON dataset"
          cp processed_data.json master_dataset.json
        else
          echo "Merging with existing JSON dataset"
          # For JSON: merge arrays (this is a simplified merge - in production might need more sophisticated merging)
          python3 -c "
import json
import sys

# Read existing dataset
with open('master_dataset.json', 'r') as f:
    existing = json.load(f)

# Read new data
with open('processed_data.json', 'r') as f:
    new_data = json.load(f)

# Simple merge - append new sessions to existing
if 'sessions' in existing and 'sessions' in new_data:
    existing['sessions'].extend(new_data['sessions'])
    existing['metadata']['total_sessions'] = len(existing['sessions'])
    existing['metadata']['last_updated'] = new_data['metadata']['generated_at']
else:
    # If structure is different, replace entirely
    existing = new_data

# Write merged dataset
with open('master_dataset.json', 'w') as f:
    json.dump(existing, f, indent=2)
    
print('JSON datasets merged successfully')
"
        fi
        
        echo "Dataset merge completed"
        echo "Final CSV size: $(wc -c < master_dataset.csv) bytes"
        echo "Final JSON size: $(wc -c < master_dataset.json) bytes"
        
    - name: Generate processing report
      run: |
        # Generate a processing report
        cat > processing_report.md << EOF
# PDF Processing Report
        
**Date:** $(date)
**File:** ${{ github.event.client_payload.filename }}
**Uploaded by:** ${{ github.event.client_payload.uploaded_by }}

## Processing Results
- âœ… PDF downloaded successfully
- âœ… Data extraction completed
- âœ… Duplicate detection performed
- âœ… Dataset backup created
- âœ… Master dataset updated

## Duplicate Detection
$(cat duplicate_report.txt)

## File Statistics
- New CSV size: $(wc -c < processed_data.csv) bytes
- New JSON size: $(wc -c < processed_data.json) bytes
- Master CSV size: $(wc -c < master_dataset.csv) bytes  
- Master JSON size: $(wc -c < master_dataset.json) bytes

## Session Summary
$(python3 -c "
import pandas as pd
try:
    df = pd.read_csv('processed_data.csv')
    print(f'- Sessions in uploaded PDF: {len(df)}')
    
    df_master = pd.read_csv('master_dataset.csv')
    print(f'- Total sessions in master dataset: {len(df_master)}')
    
    # Show date range
    if len(df_master) > 0:
        dates = pd.to_datetime(df_master['date'])
        print(f'- Dataset date range: {dates.min().strftime(\"%Y-%m-%d\")} to {dates.max().strftime(\"%Y-%m-%d\")}')
except Exception as e:
    print(f'- Error reading session information: {e}')
")
EOF
        
        echo "Processing report generated with duplicate detection results"
        
    - name: Commit updated dataset
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add files to git
        git add master_dataset.csv master_dataset.json dataset_backups/ processing_report.md
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          # Commit changes
          git commit -m "ðŸ“Š Update dataset: processed ${{ github.event.client_payload.filename }}

- Uploaded by: ${{ github.event.client_payload.uploaded_by }}
- Processing date: $(date)
- New sessions added from weekly PDF report

ðŸ¤– Automated processing via GitHub Actions"
          
          # Push changes
          git push
          
          echo "Dataset successfully updated and committed"
        fi
        
    - name: Send notification email
      env:
        RESEND_API_KEY: ${{ secrets.RESEND_API_KEY }}
      run: |
        # Send processing completion email
        curl -X POST "https://api.resend.com/emails" \
          -H "Authorization: Bearer $RESEND_API_KEY" \
          -H "Content-Type: application/json" \
          -d '{
            "from": "Cat Flap Stats <noreply@echoreflex.me>",
            "to": ["${{ github.event.client_payload.uploaded_by }}"],
            "subject": "âœ… PDF Processing Complete - ${{ github.event.client_payload.filename }}",
            "html": "<h2>PDF Processing Successful</h2><p>Your file <strong>${{ github.event.client_payload.filename }}</strong> has been processed successfully.</p><p>The master dataset has been updated and is available for download.</p><p><a href=\"https://cat-flap-stats.herrings.workers.dev/dashboard\">View Dashboard</a></p>",
            "text": "PDF Processing Complete! Your file ${{ github.event.client_payload.filename }} has been processed and the dataset has been updated. Visit the dashboard to download the latest data."
          }'
        
        echo "Notification email sent"
        
    - name: Clean up temporary files
      run: |
        # Remove temporary files
        rm -f temp_upload.pdf processed_data.csv processed_data.json
        echo "Temporary files cleaned up"